{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "PyTorch+DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "    \n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "/**Aim:**/ To maximise the cumulative reward. The reward is discounted based on time.\n",
    "\n",
    "$R_{t_{0}}= \\sum_{t=t_0}^{\\infty}\\gamma^{t-t_0}r_t$\n",
    "\n",
    "where, \n",
    "\n",
    "$ R_{t_0}$ is the *return*\n",
    "\n",
    "$\\gamma \\in (0,1)$  is the *discount* \n",
    "\n",
    "$r_t$ is the *reward at time* $t$\n",
    "\n",
    "\n",
    "/**Intuition behind Q-Learning**:/ If we had a function $Q^*: State \\times Action \\to {\\displaystyle \\mathbb {R} }$, that could tell us what our `Reward` would be for the given `State` and `Action` then we could easily construct a policy to maximize the `Reward`'s.\n",
    "\n",
    "$\\pi^*(s) = argmax_a Q^*(s,a)$\n",
    "\n",
    "In other words, for a given state, we want to maximize $Q^*$ over all possible actions and possible future states.\n",
    "\n",
    "But we dont know everything about the world implying we dont have complete access to $Q^*$.\n",
    "\n",
    "In basic Q-learning we record all `Transitions` in a Q-table which acts as our guide to maximizing reward. In DQN we use Neural Networs to resemble $Q^*$\n",
    "\n",
    "Initially, our expectation from future episodes (an episode is a Transition we defined above) wont make much sense. But as we encounter a set of states $s$, actions $a$, their resulting states $s'$ and rewards, we can make look back to make discounted extimates of rewards. The equation below represents the concept we just discussed.\n",
    "\n",
    "$Q^\\pi (s,a) = r + \\gamma Q^\\pi (s', \\pi(s'))$\n",
    "\n",
    "This is called the bellman equation.\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory and Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88e77afcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=Image.CUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "# This is based on the code from gym.\n",
    "screen_width = 600\n",
    "\n",
    "\n",
    "def get_cart_location():\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "\n",
    "def get_screen():\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))  \n",
    "    # transpose into torch order (CHW)\n",
    "    # Strip off the top and bottom of the screen\n",
    "    screen = screen[:, 160:320]\n",
    "    view_width = 320\n",
    "    cart_location = get_cart_location()\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # Strip off the edges, so that we have a square image centered on a cart\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # Convert to float, rescare, convert to torch tensor\n",
    "    # (this doesn't require a copy)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # Resize, and add a batch dimension (BCHW)\n",
    "    return resize(screen).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "policy_net = DQN().to(device)\n",
    "target_net = DQN().to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see http://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation).\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.uint8)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88a2392898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f88a2392898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # Observe new state\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # Update the target network\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
