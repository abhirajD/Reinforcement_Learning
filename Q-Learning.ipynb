{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-Learning algorithms(QLA) are Reinforment learning algorithms which are a little different than the popular policy-based algorithms.\n",
    "\n",
    "In this noteboook I wil be discussion the thoery along with a simple q-learning problem.\n",
    "\n",
    "#### TO-DO\n",
    "- Start with basic theory and discuss founding papers\n",
    "- Make an end to ed application.\n",
    "- Make a web interface to display Q-learning algorithm\n",
    "- Start with lookup-tales and go to Deep Network implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Found [this](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0) amazing article series on medium by [Arthur](https://medium.com/@awjuliani). I have chosen it to be an entry point into learnin QLA and starting this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART-0\n",
    "##### Getting an Intuition\n",
    "\n",
    "Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there.\n",
    "\n",
    "In itâ€™s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state.\n",
    "\n",
    "The lookup table would look someting like this:\n",
    "\n",
    "| State |  A1   |  A2  |  A3  |  A4  |\n",
    "|-------|-------|------|------|------|\n",
    "|  S1   |  0.0  | 0.0  |  0.7 |  1.0 |\n",
    "|  S2   |  0.5  | 0.5  |  0.5 |  0.3 |\n",
    "|  ::   |   ::  |  ::  |   :: |   :: |\n",
    "|  Sn   |  0.1  | 0.0  |  0.7 |  0.0 |\n",
    "\n",
    "- Sn  : Represents the state of the environment in which the agent lies.\n",
    "- A1-4: Represent the set of actions the agent can perform.\n",
    "- The values 0.0-1.0 of the table represent the rewad the agent has recorded for each (state, action) pair.\n",
    "- This lookup table is a construct the agent maintains within itself to interact with the world; just like we register stuff in our memory to make better interactions with the world in subsequent epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Updating the q-table\n",
    "We make updates to our Q-table using something called the [Bellman equation](https://en.wikipedia.org/wiki/Bellman_equation). The equation is widely used in Dynamic Programming to break dynamic optimizatio problems into sequence of simpler subproblems. In lay man terms the equation states that: \n",
    "\n",
    "**\\[expected long-term reward\\]** for a given action is equal to the \n",
    "**\\[immediate reward from the current action\\]** combined with the **\\[expected reward from the best future action taken at the following state\\]**.\n",
    "\n",
    "In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
